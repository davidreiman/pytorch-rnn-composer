{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        encoded = self.encoder(x)\n",
    "        y, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        y = self.decoder(y.view(batch_size, -1))\n",
    "        return y, hidden\n",
    "\n",
    "    def forward2(self, x, hidden):\n",
    "        encoded = self.encoder(x.view(1, -1))\n",
    "        y, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n",
    "        y = self.decoder(y.view(1, -1))\n",
    "        return y, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 825\n",
    "hidden_size = 32\n",
    "chunk_size = 10\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = CharRNN(vocab_size, hidden_size, vocab_size, n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(file, chunk_len, batch_size):\n",
    "    \n",
    "    file_len = len(file)\n",
    "    \n",
    "    input_ = torch.LongTensor(batch_size, chunk_len)\n",
    "    target_ = torch.LongTensor(batch_size, chunk_len)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        start = np.random.randint(file_len - chunk_len)\n",
    "        end = start + chunk_len + 1\n",
    "        \n",
    "        chunk = file[start:end]\n",
    "        \n",
    "        input_[i] = chunk[:-1]\n",
    "        target_[i] = chunk[1:]\n",
    "        \n",
    "    input_ = Variable(input_)\n",
    "    target_ = Variable(target_)\n",
    "    \n",
    "    return input_, target_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dir, epochs, batch_size, chunk_size):\n",
    "    \n",
    "    files = [os.path.join(data_dir, file) for file in \n",
    "             os.listdir(data_dir) if file.endswith('.npy')]\n",
    "    \n",
    "    pbar = tqdm(range(epochs), unit='epoch')\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        for file in files:\n",
    "            \n",
    "            f = torch.Tensor(np.load(file))\n",
    "            file_len = len(f)\n",
    "            \n",
    "            pbar2 = tqdm(range(file_len), unit='example')\n",
    "            \n",
    "            for i in pbar2:\n",
    "                \n",
    "                hidden = rnn.init_hidden(batch_size)\n",
    "                \n",
    "                error = 0\n",
    "                rnn.zero_grad()\n",
    "                \n",
    "                input_, target_ = get_batch(f, chunk_size, batch_size)\n",
    "                \n",
    "                for c in range(chunk_size):\n",
    "\n",
    "                    output, hidden = rnn(input_[:,c], hidden)\n",
    "                    error += loss(output.view(batch_size, -1), target_[:,c])\n",
    "                    \n",
    "                error.backward()\n",
    "                opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train('./data', epochs, batch_size, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
